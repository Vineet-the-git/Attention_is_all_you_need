# Attention_is_all_you_need

Self-Attention and Transformer was a complicated topic for me to understand. To understand the working of Transformers I decided to implement one from scratch.  
This is a simple but useful working implementation of a Transformer.  

I trained and tested the transformer on a classification task on IMDB dataset for classifying negative and positive movie reviews.  

These amazing blogs really helped me alot in understanding these concepts. Do refer to these--  
1. http://peterbloem.nl/blog/transformers  
2. https://towardsdatascience.com/  attention-is-all-you-need-discovering-the-transformer-paper-73e5ff5e0634  

### To train and test the model:
1. Run: `pip install -r requirements.txt`
2. Run: `cd src`
3. Run: `python classifier.py`
### References:
1. [Transformer from scratch](http://peterbloem.nl/blog/transformers)
2. [Attention is all you need: Discovering the Transformer paper](https://towardsdatascience.com/attention-is-all-you-need-discovering-the-transformer-paper-73e5ff5e0634)
3. [Attention is all you need](https://arxiv.org/abs/1706.03762)
